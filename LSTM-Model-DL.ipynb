{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Hate Speech Detection**\n",
        "This project focuses on building a model combining LSTM and Hugging Face transformer models to detect hate speech from a curated text dataset. It not only classifies whether a statement is hateful but also predicts the intensity level of the hate speech.\n",
        "https://www.kaggle.com/datasets/waalbannyantudre/hate-speech-detection-curated-dataset/data"
      ],
      "metadata": {
        "id": "5TcbLpxfhttU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LSTM Model**"
      ],
      "metadata": {
        "id": "MD8MhkBWi6hX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade kagglehub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj6PB7QIe-2Q",
        "outputId": "a9128536-3931-4b9c-bf5a-c93702cd6187"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "waalbannyantudre_hate_speech_detection_curated_dataset_path = kagglehub.dataset_download('waalbannyantudre/hate-speech-detection-curated-dataset')\n",
        "\n",
        "print('Data source import complete.')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras.layers import  Input,Dense,Embedding, LSTM,Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMpV33qWMMrf",
        "outputId": "1e64d11b-532c-4c8c-91d7-10b71692211f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enable dynamic memory growth for GPUs"
      ],
      "metadata": {
        "id": "tdSM0cVFSv-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu_device in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu_device, True)\n",
        "    except RuntimeError as e:\n",
        "        print(\"GPU Memory Growth Error:\", e)"
      ],
      "metadata": {
        "id": "nBqSqYalSZYI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "GjhzC3XvS2n6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/hate-speech-detection-curated-dataset/HateSpeechDatasetBalanced.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "uWDSxzu1SzgI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean text"
      ],
      "metadata": {
        "id": "UNxU1JNRS3tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "df['CleanedContent'] = df['Content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "tez4rYKoS6EP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Tokenization and padding"
      ],
      "metadata": {
        "id": "NpIDsw16TEl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['CleanedContent'])\n",
        "sequences = tokenizer.texts_to_sequences(df['CleanedContent'])\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "y = df['Label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "original_test_texts = df['Content'].iloc[X_test.shape[0] * -1:].tolist()\n",
        "\n",
        "# Set seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "bn_rJMaeTCUy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition"
      ],
      "metadata": {
        "id": "sQ4PmAmCTNob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(X_train.shape[1],))\n",
        "embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=20)(input_layer)\n",
        "lstm_layer1 = LSTM(10, return_sequences=True)(embedding_layer)\n",
        "lstm_layer2 = LSTM(10, return_sequences=True)(lstm_layer1)\n",
        "flatten_layer = Flatten()(lstm_layer2)\n",
        "output_layer = Dense(1, activation='sigmoid')(flatten_layer)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "metadata": {
        "id": "lnLRKbOJTL4u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile and train"
      ],
      "metadata": {
        "id": "v8frcgNjTSwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJWSyMcjTQt8",
        "outputId": "00d2ddb0-634d-4c20-c3b5-51825f9a31b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m9077/9077\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2550s\u001b[0m 280ms/step - accuracy: 0.8118 - loss: 0.4025 - val_accuracy: 0.8568 - val_loss: 0.3199\n",
            "Epoch 2/5\n",
            "\u001b[1m9077/9077\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2569s\u001b[0m 277ms/step - accuracy: 0.8775 - loss: 0.2808 - val_accuracy: 0.8650 - val_loss: 0.3169\n",
            "Epoch 3/5\n",
            "\u001b[1m9077/9077\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2511s\u001b[0m 273ms/step - accuracy: 0.8953 - loss: 0.2418 - val_accuracy: 0.8641 - val_loss: 0.3281\n",
            "Epoch 4/5\n",
            "\u001b[1m9077/9077\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2466s\u001b[0m 272ms/step - accuracy: 0.9039 - loss: 0.2230 - val_accuracy: 0.8649 - val_loss: 0.3352\n",
            "Epoch 5/5\n",
            "\u001b[1m9077/9077\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2513s\u001b[0m 277ms/step - accuracy: 0.9091 - loss: 0.2122 - val_accuracy: 0.8645 - val_loss: 0.3395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate on test data"
      ],
      "metadata": {
        "id": "Ndt2EbAPTXJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Predictions on test data\n",
        "y_pred_probs = model.predict(X_test).squeeze()\n",
        "y_pred_labels = (y_pred_probs > 0.5).astype(int)\n",
        "print(f\"\\nAccuracy Score (sklearn): {accuracy_score(y_test, y_pred_labels)}\")\n",
        "\n",
        "# Evaluate on 20 random test sentences\n",
        "print(\"\\n--- Evaluation on 20 Test Sentences ---\")\n",
        "indices = np.random.choice(len(X_test), 20, replace=False)\n",
        "for idx in indices:\n",
        "    sentence = original_test_texts[idx]\n",
        "    true_label = \"HATE\" if y_test[idx] == 1 else \"NON-HATE\"\n",
        "    prob = y_pred_probs[idx]\n",
        "    pred_label = \"HATE\" if prob > 0.5 else \"NON-HATE\"\n",
        "    intensity = round(prob * 100, 2) if pred_label == \"HATE\" else round((1 - prob) * 100, 2)\n",
        "\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"True Label: {true_label}\")\n",
        "    print(f\"Predicted Label: {pred_label}\")\n",
        "    print(f\"{'Hate' if pred_label == 'HATE' else 'Non-Hate'} Intensity: {intensity}%\")"
      ],
      "metadata": {
        "id": "j4v0RSS-TVDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b643b2c9-9361-4a20-8ace-ec924458b8e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4539/4539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 49ms/step - accuracy: 0.8639 - loss: 0.3405\n",
            "\n",
            "Test Loss: 0.33951452374458313\n",
            "Test Accuracy: 0.8644921183586121\n",
            "\u001b[1m4539/4539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 47ms/step\n",
            "\n",
            "Accuracy Score (sklearn): 0.8644920949705283\n",
            "\n",
            "--- Evaluation on 20 Test Sentences ---\n",
            "\n",
            "Sentence: stop vandalizing please stop your disruptive editing of removing rights to the unreliable and biased sources when you continue to vandalize wikipedia as you did at list of countries by population united nations and list and countries by population i will have to ask for an arbitration for examining of your access rights as well as to ask for protection of freedom and truth by blocking submissions from further editing preceding unsigned comment added by the bats are out today this must be the record for shortest time between entries by mooney editors please undo the damage you are doing to wikipedia stop doing more damage also i will have no reason to leave any more corrections on your own page if you think you are right then blocking me removing my edits and prove me wrong like this you are just showing you are arrogant and don't stand criticism of what you are doing\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 92.54000091552734%\n",
            "\n",
            "Sentence: what a hypocritical award you have i discover that you have a b wizard in christianity on your user pageboy but you called the holy scriptures folktale rather than the inspired information technology have in mind cool a k a of god it is oh the shame\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 83.51000213623047%\n",
            "\n",
            "Sentence: an ii domain binds to a effort radiate dome\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 74.75%\n",
            "\n",
            "Sentence: you lie in truth are dumb\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 77.62999725341797%\n",
            "\n",
            "Sentence: his edits do suck which is why i reverted them over and over again not one edit contributed positively to the article with most served the needs of spiritual crusaders in any case i have pointed out that my comment about him sucking what a slut reverted by me immediately it is my practice to write honestly then go back and edit myself before submitting in this i missed one spot and had to edit it after submitting as i mentioned in my edit comment since nobody read the original unless they dug through the history and since nobody ever needed to warn me about rumors your excuses are nonsense you just wanted to silence me so i could not defend myself from the ideologically motivated violation report even before you got mine badge you have made it a practice to litter my user page with spurious warnings which sometimes earned you the honor of being the only person whose text i erase on sight from my talk page you love your stalker since you are the guilty party your response here is not sufficient i would be an objective third party to respond regarding this abusive block to you\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 99.19999694824219%\n",
            "\n",
            "Sentence: i then suppose again i should not to be terribly surprised on wikipedia jews are hunted down and occasionally banned like dogs although then given in a brand of some other rather random banned user or typical racist website\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 98.83999633789062%\n",
            "\n",
            "Sentence: the notable occurrence at our borders is human trafficking and only did you know that there are many cases where daca were arrested over human trafficking and bulldthewall indiana court case level three and\n",
            "True Label: NON-HATE\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 99.25%\n",
            "\n",
            "Sentence: as you want and ignore some lump of data that shows islam to be and pile of crap it is your home\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 99.69000244140625%\n",
            "\n",
            "Sentence: damn you arrogant smart little bitch you re very willing to toss the salad i got some peanut oil and jelly in my ass waiting till you\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 87.83999633789062%\n",
            "\n",
            "Sentence: death is not fit of ask that for what without good to the intolerant rapist warmonger thief murderer the pork be upon who will have him so\n",
            "True Label: NON-HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 59.2599983215332%\n",
            "\n",
            "Sentence: ameri zone up drive sto the ramiro kara kara kara ma... yuki kaminski peace\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 62.09000015258789%\n",
            "\n",
            "Sentence: a again first give me a verse reading number that quickly shows them that isis is not openly following islam\n",
            "True Label: NON-HATE\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 80.94999694824219%\n",
            "\n",
            "Sentence: suck on hairy nipples you...\n",
            "True Label: NON-HATE\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 100.0%\n",
            "\n",
            "Sentence: and manny did not let get over beat you are fucking stupid\n",
            "True Label: HATE\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 84.81999969482422%\n",
            "\n",
            "Sentence: break fuck yourself jess bigfoot is a real name\n",
            "True Label: NON-HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 76.5999984741211%\n",
            "\n",
            "Sentence: dislike i powerfully dislike you in fact i think you are a genuine cunt\n",
            "True Label: NON-HATE\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 90.2300033569336%\n",
            "\n",
            "Sentence: ma po po vam see not yet to stance\n",
            "True Label: NON-HATE\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 99.83999633789062%\n",
            "\n",
            "Sentence: the email ok you know what sd sent me an inappropriate email i don't know how but no he sent it i now am just pissed i don't really really give a damn anymore about how unfortunate it happened so block me already you moron too thank you also goodbye for any good loser\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 99.86000061035156%\n",
            "\n",
            "Sentence: width align class no style none border solid aaa padding em margin em auto romeo style id your ability to be informed has been denied for the following reason s fuck you faggots it gives a slut a trick i only use wikipedia to rape plunder monsters and wreak havoc you name never stop me i have over computer internet home addresses to use messages handled by\n",
            "True Label: HATE\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 90.30999755859375%\n",
            "\n",
            "Sentence: if this be really cosmos you enter my yard i bequeath use my hunter rifle blow prohibited you channelize but any are in wiki so i will flag you as vandal\n",
            "True Label: NON-HATE\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict label and hate percentage for custom sentences"
      ],
      "metadata": {
        "id": "_tYcQNXxTgrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict label and hate percentage for custom sentences\n",
        "def predict_custom_sentences(sentences):\n",
        "    cleaned = [clean_text(sentence) for sentence in sentences]\n",
        "    seqs = tokenizer.texts_to_sequences(cleaned)\n",
        "    padded = pad_sequences(seqs, maxlen=max_sequence_length)\n",
        "    probs = model.predict(padded).squeeze()\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        prob = probs[i]\n",
        "        label = \"HATE\" if prob > 0.5 else \"NON-HATE\"\n",
        "        intensity = round(prob * 100, 2) if label == \"HATE\" else round((1 - prob) * 100, 2)\n",
        "        print(f\"\\nSentence: {sentence}\")\n",
        "        print(f\"Predicted Label: {label}\")\n",
        "        print(f\"{'Hate' if label == 'HATE' else 'Non-Hate'} Intensity: {intensity}%\")\n",
        "\n",
        "# Example predictions on 4 custom sentences\n",
        "custom_sentences = [\n",
        "    \"I absolutely despise those people!\",\n",
        "    \"I hope you have a wonderful day!\",\n",
        "    \"You're so stupid and annoying.\",\n",
        "    \"This is a peaceful and loving community.\"\n",
        "]\n",
        "\n",
        "predict_custom_sentences(custom_sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q362DSo6gySX",
        "outputId": "af557507-b11c-4742-eb8b-9ed0ef7326af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\n",
            "Sentence: I absolutely despise those people!\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 59.52000045776367%\n",
            "\n",
            "Sentence: I hope you have a wonderful day!\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 95.81999969482422%\n",
            "\n",
            "Sentence: You're so stupid and annoying.\n",
            "Predicted Label: HATE\n",
            "Hate Intensity: 88.68000030517578%\n",
            "\n",
            "Sentence: This is a peaceful and loving community.\n",
            "Predicted Label: NON-HATE\n",
            "Non-Hate Intensity: 64.98999786376953%\n"
          ]
        }
      ]
    }
  ]
}